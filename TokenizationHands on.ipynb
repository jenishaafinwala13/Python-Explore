{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3g9EDtJB35vP"},"outputs":[],"source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1649,"status":"ok","timestamp":1702533121654,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"z1kp4lkn4O2U","outputId":"82e0e54b-6856-4b75-8c60-504f0929977c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5742,"status":"ok","timestamp":1702533186953,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"NYrqiI-f4oL0","outputId":"be6d7ede-0e1f-43c5-9a56-a8bf076b096c"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('popular')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702533543733,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"4KdxwWu44-sN","outputId":"d8b0781a-ca63-4633-afa0-549a69a48881"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Text: Tokenization is a crucial step in natural language processing. It breaks text into individual units called tokens.\n","\n","Words:\n","['Tokenization', 'is', 'a', 'crucial', 'step', 'in', 'natural', 'language', 'processing', '.', 'It', 'breaks', 'text', 'into', 'individual', 'units', 'called', 'tokens', '.']\n","['Tokenization is a crucial step in natural language processing.', 'It breaks text into individual units called tokens.']\n"]},{"data":{"text/plain":["list"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["text = \"Tokenization is a crucial step in natural language processing. It breaks text into individual units called tokens.\"\n","words = word_tokenize(text)\n","sentences = sent_tokenize(text)\n","print(\"Original Text:\", text)\n","print(\"\\nWords:\")\n","print(words)\n","print(sentences)\n","type(sentences)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1702533630737,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"OIHfKNas6JSK","outputId":"1a98b8be-c116-4e48-b499-a4d171d4fa26"},"outputs":[{"data":{"text/plain":["list"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["type(word_tokenize(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9669,"status":"ok","timestamp":1702533906643,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"fXOOj2Kj7KK7","outputId":"38ec0d4f-c521-4390-856f-2683af4c890e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting indic-nlp-library\n","  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library)\n","  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n","Collecting sphinx-rtd-theme (from indic-nlp-library)\n","  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting morfessor (from indic-nlp-library)\n","  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.23.5)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.3.post1)\n","Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n","Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n","Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n","  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->indic-nlp-library) (1.16.0)\n","Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n","Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.5)\n","Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n","Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.4)\n","Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.9)\n","Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n","Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.13.1)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.13)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n","Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (23.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2023.11.17)\n","Installing collected packages: morfessor, sphinxcontrib-jquery, sphinx-rtd-theme, sphinx-argparse, indic-nlp-library\n","Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"]}],"source":["!pip install indic-nlp-library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":469,"status":"ok","timestamp":1702534405362,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"4YTsF9vL7iaF","outputId":"80900394-5ae2-4eb8-8fb9-ee87672d64c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["['આજે', 'બહુ', 'સરસ', 'દિવસ', 'છે', '.']\n"]}],"source":["from indicnlp.tokenize import indic_tokenize\n","gujarati = \"આજે બહુ સરસ દિવસ છે.\"\n","guj_tok = indic_tokenize.trivial_tokenize_indic(gujarati)\n","print(guj_tok)\n"]},{"cell_type":"markdown","metadata":{"id":"HbrmqWPC9qMb"},"source":["# Stemming - Removing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":457,"status":"ok","timestamp":1702537271689,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"z-YKNZNC9ug-","outputId":"c807badf-442d-4582-b34e-d6e961b93433"},"outputs":[{"name":"stdout","output_type":"stream","text":["['hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.', ':', 'hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.']\n","['hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.', ':', 'hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.']\n","['hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.', ':', 'hello', ',', 'you', 'have', 'to', 'build', 'a', 'veri', 'good', 'site', 'and', 'i', 'love', 'visit', 'your', 'site', '.']\n"]}],"source":["import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import SnowballStemmer\n","from nltk.stem import LancasterStemmer\n","\n","from nltk.tokenize import word_tokenize\n","text = \"Hello, I am trainer. How are you doing, all? I ate baked potato. Reduces\"\n","text1 = \"Hello, You have to build a very good site and I love visiting your site.  :  hello, you have to build a very good site and i love visiting your site.\"\n","words = word_tokenize(text1)\n","ps = PorterStemmer()\n","sb = SnowballStemmer('english')\n","ls = LancasterStemmer()\n","stemmed = [ps.stem(word) for word in words]\n","print(stemmed)\n","\n","stemmed1 = [sb.stem(word) for word in words]\n","print(stemmed1)\n","\n","stemmed2 = [ls.stem(word) for word in words]\n","print(stemmed1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1702539900636,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"oh0F4vmpBYlr","outputId":"3dc350d1-3955-4338-a83a-0b7b000fc629"},"outputs":[{"name":"stdout","output_type":"stream","text":["['પ્રાકૃતિક', 'ખેતી', 'કરતા', 'ખેડુતોની', 'કરેલ', 'ફિલ્ડ', 'વિઝીટનું', 'અહેવાલનું', 'પત્રક', 'માટે', 'અહીં', 'ક્લિક', 'કરો']\n","['પ્રાકૃતિક', 'ખેતી', 'કરતા', 'ખેડુતોની', 'કરેલ', 'ફિલ્ડ', 'વિઝીટનું', 'અહેવાલનું', 'પત્રક', 'માટે', 'અહીં', 'ક્લિક', 'કરો']\n"]}],"source":["from indicnlp.tokenize import indic_tokenize\n","from nltk.stem import PorterStemmer\n","\n","gujarati = \"પ્રાકૃતિક ખેતી કરતા ખેડુતોની કરેલ ફિલ્ડ વિઝીટનું અહેવાલનું પત્રક માટે અહીં ક્લિક કરો\"\n","guj_tok = indic_tokenize.trivial_tokenize_indic(gujarati)\n","print(guj_tok)\n","\n","ps = PorterStemmer()\n","stemmed = [ps.stem(word) for word in guj_tok]\n","print(stemmed)"]},{"cell_type":"markdown","metadata":{"id":"j9BSVtsBVI2-"},"source":["Lemmetization Technique :\n","Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1882,"status":"ok","timestamp":1702540008362,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"FSXie3__-FMe","outputId":"25077f58-d870-48ef-bd7d-c24619c975f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1702540545769,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"_fUwojjySv5j","outputId":"10fa29a0-9cd2-4675-a91f-facbd1122582"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hello', ',', 'You', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'I', 'love', 'visiting', 'your', 'site', '.']\n","['Hello', ',', 'You', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'I', 'love', 'visiting', 'your', 'site', '.']\n"]}],"source":["text = \"Hello, You have to build a very good site and I love visiting your site.\"\n","token_words = word_tokenize(text)\n","print(token_words)\n","lm = WordNetLemmatizer()\n","lemmas = [lm.lemmatize(_,pos='v') for _ in token_words] #verb\n","print(lemmas)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":605,"status":"ok","timestamp":1702540586300,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"gSDiWYFPUzJY","outputId":"d8e92f8f-11e6-42ea-e743-532c1d52e641"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Hello', ',', 'You', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'I', 'love', 'visiting', 'your', 'site', '.']\n","['Hello', ',', 'You', 'have', 'to', 'build', 'a', 'very', 'good', 'site', 'and', 'I', 'love', 'visiting', 'your', 'site', '.']\n"]}],"source":["text = \"Hello, You have to build a very good site and I love visiting your site.\"\n","token_words = word_tokenize(text)\n","print(token_words)\n","lm = WordNetLemmatizer()\n","lemmas = [lm.lemmatize(_,pos='n') for _ in token_words] #noun\n","print(lemmas)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1702540507528,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"U7tuRtsVUTLb","outputId":"52b64621-7cf1-4d1c-f733-5738afe78c1e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'run'"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["word = \"ran\"\n","lm.lemmatize(word,pos='v')"]},{"cell_type":"markdown","metadata":{"id":"H9fWtdQ-VN1T"},"source":["## Stop words removal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"65k80rrsV-h8"},"outputs":[],"source":["from nltk.corpus import stopwords\n","\n","token_words = word_tokenize(text)\n","token_words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyScsjJrWyPD"},"outputs":[],"source":["token_without_stopwords = [token for token in token_words if token.lower() not in stopwords.words('english')]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7138,"status":"ok","timestamp":1702542154094,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"BgwnKls5aEPI","outputId":"9452fb9c-9679-448f-822a-344941cba5ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting stanza\n","  Downloading stanza-1.7.0-py3-none-any.whl (933 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m933.2/933.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji (from stanza)\n","  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.23.5)\n","Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.2.1)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.1.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2023.11.17)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n","Installing collected packages: emoji, stanza\n","Successfully installed emoji-2.9.0 stanza-1.7.0\n"]}],"source":["pip install stanza"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d82e85a30f434f93a9760102415c24d6"]},"executionInfo":{"elapsed":2147,"status":"ok","timestamp":1702542556316,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"},"user_tz":-330},"id":"rxXZ1oN7a9sM","outputId":"a5b36d90-f398-4db9-b0bf-68bf2a1f4cdb"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d82e85a30f434f93a9760102415c24d6","version_major":2,"version_minor":0},"text/plain":["Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.7.0.json:   0%|   …"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["INFO:stanza:Loading these models for language: hi (Hindi):\n","===========================\n","| Processor | Package     |\n","---------------------------\n","| tokenize  | hdtb        |\n","| pos       | hdtb_charlm |\n","===========================\n","\n","INFO:stanza:Using device: cpu\n","INFO:stanza:Loading: tokenize\n","INFO:stanza:Loading: pos\n","INFO:stanza:Done loading processors!\n"]},{"name":"stdout","output_type":"stream","text":["[\n","  [\n","    {\n","      \"id\": 1,\n","      \"text\": \"I\",\n","      \"upos\": \"PUNCT\",\n","      \"xpos\": \"SYM\",\n","      \"start_char\": 0,\n","      \"end_char\": 1\n","    },\n","    {\n","      \"id\": 2,\n","      \"text\": \"am\",\n","      \"upos\": \"PROPN\",\n","      \"xpos\": \"NNPC\",\n","      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n","      \"start_char\": 2,\n","      \"end_char\": 4\n","    },\n","    {\n","      \"id\": 3,\n","      \"text\": \"girl.\",\n","      \"upos\": \"PUNCT\",\n","      \"xpos\": \"SYM\",\n","      \"start_char\": 5,\n","      \"end_char\": 10\n","    },\n","    {\n","      \"id\": 4,\n","      \"text\": \"I\",\n","      \"upos\": \"PROPN\",\n","      \"xpos\": \"NNPC\",\n","      \"start_char\": 11,\n","      \"end_char\": 12\n","    },\n","    {\n","      \"id\": 5,\n","      \"text\": \"love\",\n","      \"upos\": \"PROPN\",\n","      \"xpos\": \"NNPC\",\n","      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n","      \"start_char\": 13,\n","      \"end_char\": 17\n","    },\n","    {\n","      \"id\": 6,\n","      \"text\": \"to\",\n","      \"upos\": \"PROPN\",\n","      \"xpos\": \"NNPC\",\n","      \"start_char\": 18,\n","      \"end_char\": 20\n","    },\n","    {\n","      \"id\": 7,\n","      \"text\": \"Dance\",\n","      \"upos\": \"PROPN\",\n","      \"xpos\": \"NNP\",\n","      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3\",\n","      \"start_char\": 21,\n","      \"end_char\": 26\n","    }\n","  ]\n","]\n"]}],"source":["import stanza\n","pl=stanza.Pipeline('hi',processors='tokenize,pos')\n","\n","words=\"I am girl. I love to Dance\"\n","processed_text=pl(words)\n","print (processed_text)"]},{"cell_type":"markdown","source":["Bag of words"],"metadata":{"id":"hKcmjVut1Fch"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from collections import Counter\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","text_data = \"This is a simple example of a Bag of Words model. Bag of Words is a text preprocessing technique in NLP.\"\n","\n","# Tokenize the text into words\n","words = word_tokenize(text_data.lower())  # Convert to lowercase for consistency\n","\n","# Remove stop words\n","stop_words = set(stopwords.words('english'))\n","filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n","\n","# Create a Bag of Words model\n","bow_model = Counter(filtered_words)\n","\n","# Print the Bag of Words\n","print(\"Original Text:\")\n","print(text_data)\n","print(\"\\nBag of Words Model:\")\n","print(bow_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hly5uv974S3l","executionInfo":{"status":"ok","timestamp":1702627534173,"user_tz":-330,"elapsed":4106,"user":{"displayName":"JENISHA TAILOR","userId":"01089825184898520478"}},"outputId":"3aa24286-17fe-47fa-f57c-36069c9a0098"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Original Text:\n","This is a simple example of a Bag of Words model. Bag of Words is a text preprocessing technique in NLP.\n","\n","Bag of Words Model:\n","Counter({'bag': 2, 'words': 2, 'simple': 1, 'example': 1, 'model': 1, 'text': 1, 'preprocessing': 1, 'technique': 1, 'nlp': 1})\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM9U3y39uOa7Cce84hq1rNV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}